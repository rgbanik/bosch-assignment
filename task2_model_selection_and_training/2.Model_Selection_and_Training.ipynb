{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF_yIbiXGisU"
      },
      "source": [
        "# Task 2.2: Training the model\n",
        "\n",
        "In this notebook, I convert the BDD100k labels to COCO format, train a Faster R-CNN ResNet-50 FPN network for 5 epochs, and save the model weights locally.\n",
        "\n",
        "Unfortunately, you will not be able to run this without installing all the necessary packages and making some serious adjustments to the paths. \n",
        "\n",
        "Please read along, as I have tried my best to explain via markdown and comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfRmrWCGYjVo"
      },
      "source": [
        "### The first step is to convert the annotations into COCO format. I did this by hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7PQ-HDmobhK"
      },
      "outputs": [],
      "source": [
        "# Since we already know the classes from analysis time, let's just\n",
        "# hardcode a dict to avoid counting everything again\n",
        "\n",
        "# Here I have made a mistake for which I have paid for dearly during evaluation\n",
        "# The indices should start at 1, but I am leaving it as-is to demonstrate what I have done\n",
        "category_ids = {\n",
        "    \"traffic sign\": 0,\n",
        "    \"traffic light\": 1,\n",
        "    \"car\": 2,\n",
        "    \"rider\": 3,\n",
        "    \"motor\": 4, # Motorcycle, haha\n",
        "    \"person\": 5,\n",
        "    \"bus\": 6,\n",
        "    \"truck\": 7,\n",
        "    \"bike\": 8,\n",
        "    \"train\": 9\n",
        "}\n",
        "\n",
        "# Also, the image size is fixed for the whole dataset\n",
        "IMAGE_WIDTH = 1280\n",
        "IMAGE_HEIGHT = 720\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xZimYvflhT2s"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from itertools import islice # Needed to create subset\n",
        "import json\n",
        "\n",
        "def convert_bdd_to_coco(bdd_json, config):\n",
        "    split, size = config # Not using size in this run\n",
        "    print(f\"Starting conversion for BDD {split} labels.\")\n",
        "    coco_json = {\n",
        "        \"type\": \"instances\"\n",
        "    }\n",
        "\n",
        "    # Setting the categories attribute is straightforward,\n",
        "    # since we have already assigned ids to each of the\n",
        "    # 10 classes, and they have no supercategory\n",
        "    coco_json[\"categories\"] = [\n",
        "        {\n",
        "            \"id\": id,\n",
        "            \"name\": category_name,\n",
        "            \"supercategory\": \"none\"\n",
        "        }\n",
        "        for category_name, id in category_ids.items()\n",
        "    ]\n",
        "\n",
        "    # Now, we shall iterate over each of the objects of the\n",
        "    # bdd_json, and populate the images and annotations keys\n",
        "    # of our coco_json object\n",
        "    coco_json[\"images\"] = []\n",
        "    coco_json[\"annotations\"] = []\n",
        "    for img_index, obj in enumerate(tqdm(bdd_json, total=len(bdd_json))):\n",
        "        image_obj = {\n",
        "            \"file_name\": obj[\"name\"],\n",
        "            \"height\": IMAGE_HEIGHT,\n",
        "            \"width\": IMAGE_WIDTH,\n",
        "            \"id\": img_index\n",
        "        }\n",
        "        image_has_valid_labels = False\n",
        "        for label in obj[\"labels\"]:\n",
        "            if label[\"category\"] in category_ids.keys():\n",
        "                image_has_valid_labels = True\n",
        "                x1 = label[\"box2d\"][\"x1\"]\n",
        "                y1 = label[\"box2d\"][\"y1\"]\n",
        "                x2 = label[\"box2d\"][\"x2\"]\n",
        "                y2 = label[\"box2d\"][\"y2\"]\n",
        "                # Build annotation object from extracted information\n",
        "                annotation = {\n",
        "                    \"id\": label[\"id\"],\n",
        "                    \"image_id\": img_index,\n",
        "                    \"category_id\": category_ids[label[\"category\"]],\n",
        "                    \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
        "                    \"area\": float((x2 - x1) * (y2 - y1)),\n",
        "                    \"iscrowd\": 0,\n",
        "                    \"ignore\": 0,\n",
        "                    \"segmentation\": [x1, y1, x1, y2, x2, y2, x2, y1]\n",
        "                }\n",
        "                coco_json[\"annotations\"].append(annotation)\n",
        "        if image_has_valid_labels:\n",
        "            coco_json[\"images\"].append(image_obj)\n",
        "\n",
        "    # Finally, write the coco_json to a label file\n",
        "    with open(f\"/home/ghosh/content/data/labels_coco/{split}.json\", \"w\") as f:\n",
        "        json.dump(coco_json, f)\n",
        "    print(f\"Finished conversion for BDD {split} labels.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMiKd2j2pLeT",
        "outputId": "e6a762f3-8cf8-4b05-9835-18e042c8becd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting conversion for BDD train labels.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 69863/69863 [00:03<00:00, 21088.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished conversion for BDD train labels.\n",
            "Starting conversion for BDD val labels.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 54703.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished conversion for BDD val labels.\n"
          ]
        }
      ],
      "source": [
        "# Element at index 1 of tuple is only required when using a subset\n",
        "# of the data. In this case, the subset is 10%. But I won't use it.\n",
        "configs = [(\"train\", 7000), (\"val\", 1000)]\n",
        "for config in configs:\n",
        "    # Read the bdd label json and generate coco labels\n",
        "    bdd_json_path = f\"/home/ghosh/content/data/labels_json/bdd100k_labels_images_{config[0]}.json\"\n",
        "    with open(bdd_json_path, \"r\") as f:\n",
        "        bdd_json = json.load(f)\n",
        "    convert_bdd_to_coco(bdd_json, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwGF_ZlbtF91"
      },
      "source": [
        "### Now that the labels have been converted, let's move onto setting things up for training. First we import necessary modules and initialize some paths variables and constants for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0ySrnyuv0Sz",
        "outputId": "41f15bc5-1343-4a13-d7d9-837a07f1ef47"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n",
        "from torchvision.transforms import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Let's initialize our paths\n",
        "# For training set\n",
        "train_images_path = \"/home/ghosh/content/data/images/train\"\n",
        "train_labels_coco_path = \"/home/ghosh/content/data/labels_coco/train.json\"\n",
        "# For validation set\n",
        "val_images_path = \"/home/ghosh/content/data/images/val\"\n",
        "val_labels_coco_path = \"/home/ghosh/content/data/labels_coco/val.json\"\n",
        "\n",
        "# And set some hyperparameters\n",
        "BATCH_SIZE = 4 # Small batch size due to time and memory constraints\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0005\n",
        "MOMENTUM = 0.9\n",
        "NUM_EPOCHS = 5 # Few epochs due to time and memory constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "j3ZuwFVdgrU5",
        "outputId": "7935942f-ec1f-406b-8eaa-bdc5b6edcb65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ghosh/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manik-ghosh\u001b[0m (\u001b[33manik-ghosh-rwth-aachen-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ghosh/content/wandb/run-20250924_095319-44y3jiwo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/anik-ghosh-rwth-aachen-university/bosch-assignment/runs/44y3jiwo' target=\"_blank\">faster-rcnn-train</a></strong> to <a href='https://wandb.ai/anik-ghosh-rwth-aachen-university/bosch-assignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/anik-ghosh-rwth-aachen-university/bosch-assignment' target=\"_blank\">https://wandb.ai/anik-ghosh-rwth-aachen-university/bosch-assignment</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/anik-ghosh-rwth-aachen-university/bosch-assignment/runs/44y3jiwo' target=\"_blank\">https://wandb.ai/anik-ghosh-rwth-aachen-university/bosch-assignment/runs/44y3jiwo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/anik-ghosh-rwth-aachen-university/bosch-assignment/runs/44y3jiwo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f2eabac5810>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "import importlib\n",
        "\n",
        "with open(\"/home/ghosh/content/wandb_api_key.txt\", \"r\") as f:\n",
        "    wandb_api_key = f.read().strip()\n",
        "\n",
        "wandb.login(key=wandb_api_key)\n",
        "wandb.init(\n",
        "    project=\"bosch-assignment\",\n",
        "    name=\"faster-rcnn-train\",\n",
        "    config={\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"momentum\": MOMENTUM,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"num_epochs\": NUM_EPOCHS\n",
        "    },\n",
        "    reinit=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that wandb has been set up, lets define a function to create a data loader for our train and test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "663V4U5CJVuF"
      },
      "outputs": [],
      "source": [
        "# Create transform for dataloader\n",
        "class COCOTransform:\n",
        "    def __call__(self, image, target):\n",
        "        # Convert the PIL image to a tensor\n",
        "        image_tensor = F.to_tensor(image)\n",
        "\n",
        "        # Extract labels and correspondingbounding boxes\n",
        "        labels = []\n",
        "        bounding_boxes = []\n",
        "        for obj in target:\n",
        "            x, y, w, h = obj[\"bbox\"]\n",
        "            labels.append(obj[\"category_id\"])\n",
        "            bounding_boxes.append([x, y, x + w, y + h])\n",
        "\n",
        "        # Return image as a tensor, and dict of labels and bounding boxes\n",
        "        return image_tensor, {\n",
        "            \"boxes\": torch.tensor(bounding_boxes, dtype=torch.float32),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TpT0BV6269lu"
      },
      "outputs": [],
      "source": [
        "# Function with lots of boilerplate for creating\n",
        "# dataloaders for training and validation set\n",
        "def get_train_and_val_dataloaders(\n",
        "        train_images_path,\n",
        "        train_labels_coco_path,\n",
        "        val_images_path,\n",
        "        val_labels_coco_path,\n",
        "        batch_size=4\n",
        "    ):\n",
        "    train_dataset = CocoDetection(\n",
        "        root=train_images_path,\n",
        "        annFile=train_labels_coco_path,\n",
        "        transforms=COCOTransform()\n",
        "    )\n",
        "\n",
        "    val_dataset = CocoDetection(\n",
        "        root=val_images_path,\n",
        "        annFile=val_labels_coco_path,\n",
        "        transforms=COCOTransform()\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        collate_fn=lambda batch: tuple(zip(*batch))\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        collate_fn=lambda batch: tuple(zip(*batch))\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we create a function to setup our model, which is Faster R-CNN with a ResNet-50 backbone + Feature Pyramid Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9doRR7w635ud"
      },
      "outputs": [],
      "source": [
        "def setup_model(device):\n",
        "    # For this assignment, I picked Faster R-CNN with ResNet50 + FPN\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
        "        weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "    )\n",
        "    # Setting final MLP size\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(\n",
        "        model.roi_heads.box_predictor.cls_score.in_features,\n",
        "        11 # Since we have 10 classes in BDD100k + 1 for background\n",
        "    )\n",
        "    # Use CPU if GPU not available\n",
        "    model.to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def setup_optimizer_and_scheduler(model):\n",
        "    # References of the model weights that need to be updated during training\n",
        "    model_weights = [\n",
        "        param for param in model.parameters()\n",
        "        if param.requires_grad\n",
        "    ]\n",
        "\n",
        "    # Stochastic Gradient Descent with Momentum and Weight Decay\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params=model_weights,\n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        momentum=MOMENTUM\n",
        "    )\n",
        "\n",
        "    # To dynamically adjust learning rate for more stable training\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=3,\n",
        "        gamma=0.1\n",
        "    )\n",
        "\n",
        "    return optimizer, lr_scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In each epoch, the images and labels will be moved to the same device as the model, and then the output of the model will be computed for the images, which will then be compared to the ground truth to compute the loss. \n",
        "\n",
        "Based on the loss, the gradients will be computed, and the weights of the model will be updated using Stochastic Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW9Pz_zD0ChY"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, train_loader, device, epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = [\n",
        "            {\n",
        "                key: value.to(device)\n",
        "                for key, value in target.items()\n",
        "            }\n",
        "            for target in targets\n",
        "        ]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Set any accumulated gradients to zero as a just-in-case\n",
        "        optimizer.zero_grad()\n",
        "        # Backpropagate\n",
        "        losses.backward()\n",
        "        # Update weights with backpropagated gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    # Print and log to wandb\n",
        "    print(f\"Epoch {epoch} loss: {epoch_loss / len(train_loader)}\")\n",
        "    wandb.log({\"train_loss\": epoch_loss / len(train_loader)}, step=epoch)\n",
        "\n",
        "    # Save weights at the end of each epoch to load later for evaluation\n",
        "    local_weights_path = f\"/home/ghosh/content/model_weights/faster_rcnn_weights_ep{epoch}.pth\"\n",
        "    torch.save(model.state_dict(), local_weights_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation function.  \n",
        "\n",
        "Although this is part of the third task, I just wanted to show that I know how to use a validation set. \n",
        "\n",
        "I won't be tuning hyperparameters, but I feel that it is necessary to demonstrate the model's performance at the end of the epoch on an unseen dataset. Using torch.no_grad() ensures that the weights aren't polluted by the validation set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU9GwH8xPrYc"
      },
      "outputs": [],
      "source": [
        "def evaluate_on_validation_set(model, val_loader, device, epoch):\n",
        "    model.eval()\n",
        "    metric = MeanAveragePrecision()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in val_loader:\n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [\n",
        "                {\n",
        "                    key: value.to(device)\n",
        "                    for key, value in target.items()\n",
        "                }\n",
        "                for target in targets\n",
        "            ]\n",
        "            outputs = model(images)\n",
        "            metric.update(outputs, targets)\n",
        "\n",
        "    result = metric.compute()\n",
        "    output_string = (\n",
        "        f\"Validation results for epoch {epoch}: mAP = {result['map'].item()} | \"\n",
        "        f\"mAP@0.5 = {result['map_50'].item()} | \"\n",
        "        f\"mAP@0.75 = {result['map_75'].item()} | \"\n",
        "    )\n",
        "\n",
        "    # Print and log to wandb\n",
        "    print(output_string)\n",
        "    wandb.log({\n",
        "        \"val_mAP\": result[\"map\"].item(),\n",
        "        \"val_mAP@0.5\": result[\"map_50\"].item(),\n",
        "        \"val_mAP@0.75\": result[\"map_75\"].item(),\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now for the final block: the training loop! \n",
        "\n",
        "Several variables and constants being used here have been defined in the imports block above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vCbXweFS_WI",
        "outputId": "c0421c73-2c07-4345-c00a-56c5c09b1266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=5.62s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=1.28s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 17466/17466 [2:06:02<00:00,  2.31it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 loss: 0.8207084674148861\n",
            "Validation results for epoch 1: mAP = 0.21971940994262695 | mAP@0.5 = 0.42661821842193604 | mAP@0.75 = 0.19748111069202423 | \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 17466/17466 [2:03:49<00:00,  2.35it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 loss: 0.7637807072995455\n",
            "Validation results for epoch 2: mAP = 0.22688241302967072 | mAP@0.5 = 0.43382614850997925 | mAP@0.75 = 0.2058487832546234 | \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 17466/17466 [2:03:50<00:00,  2.35it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 loss: 0.747370448849475\n",
            "Validation results for epoch 3: mAP = 0.2351963371038437 | mAP@0.5 = 0.44903817772865295 | mAP@0.75 = 0.210839182138443 | \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 17466/17466 [2:03:52<00:00,  2.35it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 loss: 0.714674378115738\n",
            "Validation results for epoch 4: mAP = 0.24171017110347748 | mAP@0.5 = 0.45656388998031616 | mAP@0.75 = 0.22230303287506104 | \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 17466/17466 [2:03:45<00:00,  2.35it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 loss: 0.7095464302251856\n",
            "Validation results for epoch 5: mAP = 0.24161143600940704 | mAP@0.5 = 0.4578508734703064 | mAP@0.75 = 0.22035889327526093 | \n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_loader, val_loader = get_train_and_val_dataloaders(\n",
        "    train_images_path,\n",
        "    train_labels_coco_path,\n",
        "    val_images_path,\n",
        "    val_labels_coco_path,\n",
        "    BATCH_SIZE\n",
        ")\n",
        "model = setup_model(device)\n",
        "optimizer, lr_scheduler = setup_optimizer_and_scheduler(model)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_one_epoch(model, optimizer, train_loader, device, epoch+1)\n",
        "    evaluate_on_validation_set(model, val_loader, device, epoch+1)\n",
        "    lr_scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH8E_Ab8R5as"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fasterrcnn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
